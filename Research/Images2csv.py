# -*- coding: utf-8 -*-
"""images2csv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q2f8x96X-exHPZr15HA3DMFa3W5bhNiW
"""

import os
import pandas as pd
import numpy as np
import csv
import traceback
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline

# Create directories if they do not exist
output_dir = 'output1'
log_file_path = os.path.join(output_dir, 'error_log.csv')
results_file_path = os.path.join(output_dir, 'results.csv')
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Initialize log list
log_entries = []

# Function to process a single file
def process_file(file_path):
    try:
        # Read CSV data
        data = pd.read_csv(file_path)

        # Convert all columns containing '%' to numeric
        for col in data.columns:
            if data[col].dtype == 'object' and data[col].str.contains('%').any():
                data[col] = data[col].str.replace('%', '').astype(float)

        # Feature Engineering
        data['MarketCap Per Minute'] = data['Market Cap'] / (12 * 60)
        data['Volume Per Minute'] = data['Volume'] / (12 * 60)
        data['Market Force Indicator'] = (data['Volume Per Minute']) / (data['MarketCap Per Minute'])
        data['Volume_Float_Ratio'] = data['Volume'] / data['Shares Float']

        # Feature Selection
        features = ['Market Cap', 'P/E', 'Volume', 'Price', 'MarketCap Per Minute',
                    'Volume Per Minute', 'Market Force Indicator', 'Float Short',
                    'Short Ratio', 'Short Interest', 'Low', 'High', 'Open',
                    'Relative Volume', 'Average True Range', 'Volatility (Week)',
                    'Volatility (Month)', 'Insider Ownership', 'Insider Transactions',
                    'Institutional Ownership', 'Institutional Transactions',
                    'Volume_Float_Ratio']
        X = data[features].fillna(0)
        y = data['Change']

        # Normalize and prepare data
        scaler = StandardScaler()
        poly = PolynomialFeatures(degree=2, include_bias=False)
        gb_model = GradientBoostingRegressor(random_state=42)
        pipeline = Pipeline([
            ('scaler', scaler),
            ('poly', poly),
            ('gb', gb_model)
        ])

        # Hyperparameter tuning
        param_distributions = {
            'gb__n_estimators': [100, 200],
            'gb__max_depth': [3, 4, 5],
            'gb__learning_rate': [0.01, 0.1, 0.2],
            'gb__subsample': [0.8, 1.0],
            'gb__min_samples_split': [2, 5],
            'gb__min_samples_leaf': [1, 2, 4]
        }
        tscv = TimeSeriesSplit(n_splits=5)
        random_search = RandomizedSearchCV(estimator=pipeline, param_distributions=param_distributions,
                                           n_iter=20, cv=tscv, n_jobs=-1, verbose=2, random_state=42)

        # Train model
        random_search.fit(X, y)
        best_pipeline = random_search.best_estimator_

        # Make predictions and evaluate model
        y_pred_best = best_pipeline.predict(X)
        mse_best = mean_squared_error(y, y_pred_best)
        r2_best = r2_score(y, y_pred_best)

        # Get feature importances
        feature_importances = best_pipeline.named_steps['gb'].feature_importances_
        fitted_poly = best_pipeline.named_steps['poly']
        all_feature_names = fitted_poly.get_feature_names_out(features)
        importances_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importances})
        importances_df = importances_df.sort_values(by='Importance', ascending=False)

        # Extract top 25 feature importances
        top_importances_df = importances_df.head(25)

        # Compile results for the current file
        result_row = {
            'File': file_path,
            'Best Parameters': random_search.best_params_,
            'Best Score': random_search.best_score_,
            'Mean Squared Error': mse_best,
            'R^2 Score': r2_best
        }

        # Add top 25 features and their importances
        for i, (feature, importance) in enumerate(zip(top_importances_df['Feature'], top_importances_df['Importance']), start=1):
            result_row[f'Top {i} Feature'] = feature
            result_row[f'Top {i} Importance'] = importance

        return result_row

    except Exception as e:
        error_message = f"Error processing file {file_path}: {e}\n{traceback.format_exc()}"
        log_entries.append({'File': file_path, 'Error': error_message})
        print(error_message)
        return None

# Process all files in the csv directory
csv_dir = 'csv'
files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]

# Initialize list for all results
all_results = []

for file in files:
    file_path = os.path.join(csv_dir, file)
    result_row = process_file(file_path)
    if result_row:
        all_results.append(result_row)

# Save results to a CSV file
results_df = pd.DataFrame(all_results)
results_df.to_csv(results_file_path, index=False)
print(f"Results saved as '{results_file_path}'")

# Save error logs to CSV
with open(log_file_path, mode='w', newline='') as log_file:
    log_writer = csv.DictWriter(log_file, fieldnames=['File', 'Error'])
    log_writer.writeheader()
    log_writer.writerows(log_entries)
print(f"Error log saved as '{log_file_path}'")