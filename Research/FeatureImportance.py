# -*- coding: utf-8 -*-
"""FeatureImportance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RZEu4pAZ6SPW3QrBgoRCExVP2kwvN4JB
"""

import datetime
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from enum import Enum
from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
import joblib

# Load your CSV data
data = pd.read_csv('finviz (3).csv')
# Convert all columns containing '%' to numeric
for col in data.columns:
    if data[col].dtype == 'object' and data[col].str.contains('%').any():
        data[col] = data[col].str.replace('%', '').astype(float)

# Feature Engineering: Calculate MarketCap Per Minute and Volume Per Minute
data['MarketCap Per Minute'] = data['Market Cap'] / (12 * 60)
data['Volume Per Minute'] = data['Volume'] / (12 * 60)

# Calculate Market Force Indicator
data['Market Force Indicator'] = (data['Volume Per Minute']) / (data['MarketCap Per Minute'])

# Display the head of the dataframe
print(data.head())

# Descriptive Statistics
print(data.describe())

# Select only numeric columns for correlation matrix
numeric_cols = data.select_dtypes(include=[np.number])

# Correlation Matrix
correlation_matrix = numeric_cols.corr()

# Visualization of the Correlation Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()
# Calculate volume/float ratio
data['Volume_Float_Ratio'] = data['Volume'] / data['Shares Float']

# Select relevant features including the new ratio
features = ['Market Cap', 'P/E', 'Volume', 'Price', 'MarketCap Per Minute',
            'Volume Per Minute', 'Market Force Indicator', 'Float Short',
            'Short Ratio', 'Short Interest', 'Low', 'High', 'Open',
            'Relative Volume', 'Average True Range', 'Volatility (Week)',
            'Volatility (Month)', 'Insider Ownership', 'Insider Transactions',
            'Institutional Ownership', 'Institutional Transactions',
            'Volume_Float_Ratio']  # Add Volume_Float_Ratio here

# Define X (features) and y (target)
X = data[features]
y = data['Change']  # Assuming 'Change' is your target variable

# Split data into training and testing sets (adjust as needed)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

target = 'Change'

# Check if all features are present in the DataFrame and warn if not
missing_features = [f for f in features if f not in data.columns]
if missing_features:
    print(f"Warning: The following features are not in the DataFrame: {missing_features}")

# Use only the features that are present
features = [f for f in features if f in data.columns]

print(data.columns)
print(data.head())

X = data[features].fillna(0)  #Fill NA values with 0 or use a better strategy
y = data[target]


# Normalize the features
scaler = StandardScaler()

# Polynomial Features
poly = PolynomialFeatures(degree=2, include_bias=False)

# Initialize and train a Gradient Boosting model
gb_model = GradientBoostingRegressor(random_state=42)

# Create a pipeline
pipeline = Pipeline([
    ('scaler', scaler),
    ('poly', poly),
    ('gb', gb_model)
])

# Hyperparameter tuning with Randomized Search
param_distributions = {
    'gb__n_estimators': [100, 200],
    'gb__max_depth': [3, 4, 5],
    'gb__learning_rate': [0.01, 0.1, 0.2],
    'gb__subsample': [0.8, 1.0],
    'gb__min_samples_split': [2, 5],
    'gb__min_samples_leaf': [1, 2, 4]
}

# Use TimeSeriesSplit for cross-validation
tscv = TimeSeriesSplit(n_splits=5)
random_search = RandomizedSearchCV(estimator=pipeline, param_distributions=param_distributions, n_iter=20, cv=tscv, n_jobs=-1, verbose=2, random_state=42)
random_search.fit(X, y)

# Print the best parameters and the best score
print(f'Best Parameters: {random_search.best_params_}')
print(f'Best Score: {random_search.best_score_}')

# Use the best estimator to make predictions
best_pipeline = random_search.best_estimator_
y_pred_best = best_pipeline.predict(X)

# Evaluate the best model
mse_best = mean_squared_error(y, y_pred_best)
r2_best = r2_score(y, y_pred_best)

print(f'Best Model Mean Squared Error: {mse_best}')
print(f'Best Model R^2 Score: {r2_best}')

# Residual Analysis
residuals = y - y_pred_best
plt.figure(figsize=(10, 6))
plt.scatter(y, residuals)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Actual Change')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

# Feature Importance
feature_importances = best_pipeline.named_steps['gb'].feature_importances_

# Get the fitted PolynomialFeatures from the pipeline
fitted_poly = best_pipeline.named_steps['poly']

# Use all feature names from the fitted PolynomialFeatures
all_feature_names = fitted_poly.get_feature_names_out(features)

importances_df = pd.DataFrame({
    'Feature': all_feature_names,
    'Importance': feature_importances
})

# Sort by importance
importances_df = importances_df.sort_values(by='Importance', ascending=False)

top_n = 25  # Change this to 15 if you want top 15
top_importances_df = importances_df.head(top_n)

plt.figure(figsize=(10, 8))
sns.barplot(data=top_importances_df, x='Importance', y='Feature')
plt.title(f'Top {top_n} Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()


# Visualize Predictions vs Actual
plt.figure(figsize=(10, 6))
plt.scatter(y, y_pred_best)
plt.plot([min(y), max(y)], [min(y), max(y)], color='red', linestyle='--')
plt.xlabel('Actual Change')
plt.ylabel('Predicted Change')
plt.title('Actual vs Predicted Change')
plt.show()

class Constant(Enum):
    STOCK = 'stock'
    CURRENCY = 'currency'

class FinViz:
    ticker_list = ["EURUSD", "GBPUSD", "USDJPY", "USDCAD", "USDCHF", "AUDUSD", "NZDUSD", "EURGBP", "GBPJPY", "BTCUSD", "TOP"]
    time_frame_list = ["i1", "i3", "i5", "h", "d", "w", "m"]
    URL = "https://elite.finviz.com/api/quote.ashx?instrument=forex&rev=356737"

    def __init__(self, asset_type: str, timeout: int = 10, time_frame: str = 'i1') -> None:
        self.time_frame = time_frame
        self.asset_type = asset_type
        self._timeout = timeout
        if asset_type == Constant.STOCK:
            self.URL = "https://elite.finviz.com/api/quote.ashx?instrument=stock&rev=356737"
        else:
            self.URL = "https://elite.finviz.com/api/quote.ashx?instrument=forex&rev=356737"

    def get_all_data(self, time_frame: str = "i1", ticker: str = "EURUSD"):
        if time_frame not in self.time_frame_list:
            raise Exception("Incorrect time frame specified, please choose from the list", FinViz.time_frame_list)

        payload = {"ticker": ticker, "timeframe": time_frame, "type": "new"}
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}
        response = requests.get(self.URL, params=payload, timeout=self._timeout, headers=headers)

        if response.status_code != 200:
            print(f'asset type: {self.asset_type}')
            raise Exception("No response from server")

        json_response = response.json()
        all_volumes = [float(i) for i in json_response["volume"]]
        all_opens = [float(i) for i in json_response["open"]]
        all_closes = [float(i) for i in json_response["close"]]
        all_dates = [float(i) for i in json_response["date"]]

        return all_volumes, all_opens, all_closes, all_dates

class DivisionFeatures(BaseEstimator, TransformerMixin):
    def __init__(self, features_to_divide):
        self.features_to_divide = features_to_divide

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_new = pd.DataFrame()
        for i in range(len(self.features_to_divide)):
            for j in range(i+1, len(self.features_to_divide)):
                feature1 = self.features_to_divide[i]
                feature2 = self.features_to_divide[j]
                if feature1 in X.columns and feature2 in X.columns:
                    new_feature_name = f"{feature1}_div_{feature2}"
                    X_new[new_feature_name] = X[feature1] / (X[feature2] + 1e-8)
                    new_feature_name = f"{feature2}_div_{feature1}"
                    X_new[new_feature_name] = X[feature2] / (X[feature1] + 1e-8)
        return X_new

class MultiplicationFeatures(BaseEstimator, TransformerMixin):
    def __init__(self, features_to_multiply):
        self.features_to_multiply = features_to_multiply

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_new = pd.DataFrame()
        for i in range(len(self.features_to_multiply)):
            for j in range(i+1, len(self.features_to_multiply)):
                feature1 = self.features_to_multiply[i]
                feature2 = self.features_to_multiply[j]
                if feature1 in X.columns and feature2 in X.columns:
                    new_feature_name = f"{feature1}_mul_{feature2}"
                    X_new[new_feature_name] = X[feature1] * X[feature2]
        return X_new

def percentage_to_float(value):
    if isinstance(value, str):
        value = value.replace(',', '')
        if '%' in value:
            return float(value.strip('%')) / 100
        else:
            try:
                return float(value)
            except ValueError:
                return np.nan
    return value

class FeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, features):
        self.features = features

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_new = X[self.features].copy()

        for i in range(len(self.features)):
            for j in range(i+1, len(self.features)):
                feature1, feature2 = self.features[i], self.features[j]
                if feature1 in X.columns and feature2 in X.columns:
                    X_new[f"{feature1}_div_{feature2}"] = X[feature1] / (X[feature2] + 1e-8)
                    X_new[f"{feature2}_div_{feature1}"] = X[feature2] / (X[feature1] + 1e-8)
                    X_new[f"{feature1}_mul_{feature2}"] = X[feature1] * X[feature2]

        return X_new

def analyze_feature_importance(X, y, features, feature_type='all'):
    if feature_type == 'original':
        X_transformed = X[features]
    else:
        feature_transformer = FeatureTransformer(features)
        X_transformed = feature_transformer.transform(X)

        if feature_type == 'division':
            X_transformed = X_transformed[[col for col in X_transformed.columns if '_div_' in col]]
        elif feature_type == 'multiplication':
            X_transformed = X_transformed[[col for col in X_transformed.columns if '_mul_' in col]]
        # 'all' type will use all features, so no need for additional filtering

    X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)

    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('gb', GradientBoostingRegressor(random_state=42))
    ])

    pipeline.fit(X_train, y_train)

    y_pred = pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"\nModel Performance ({feature_type} features):")
    print(f"Mean Squared Error: {mse}")
    print(f"R^2 Score: {r2}")

    feature_importances = pipeline.named_steps['gb'].feature_importances_
    feature_names = X_transformed.columns

    importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
    importances_df = importances_df.sort_values(by='Importance', ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(data=importances_df.head(25), x='Importance', y='Feature')
    plt.title(f'Top 25 Feature Importances ({feature_type.capitalize()} Features)')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()

    print(f"\nTop 25 Most Important {feature_type.capitalize()} Features:")
    for i, (feature, importance) in enumerate(zip(importances_df['Feature'].head(25), importances_df['Importance'].head(25)), 1):
        print(f"{i}. {feature}: {importance:.4f}")

    return importances_df

def prepare_training_data():
    data = pd.read_csv('finviz (3).csv')

    all_features = [
        'Volume', 'Relative Volume', 'Market Cap',
        'Float Short', 'Short Ratio', 'Average True Range',
        'Volatility (Week)', 'Volatility (Month)',
        '20-Day Simple Moving Average', '50-Day Simple Moving Average',
        'Price', 'Open', 'High', 'Low',
        'Performance (1 Minute)', 'Performance (5 Minutes)', 'Performance (15 Minutes)',
        'Performance (1 Hour)', 'Performance (Week)'
    ]

    features = [f for f in all_features if f in data.columns]

    for col in features + ['Change']:
        if col in data.columns:
            data[col] = data[col].apply(percentage_to_float)

    X = data[features].apply(pd.to_numeric, errors='coerce')
    y = data['Change'].apply(pd.to_numeric, errors='coerce')

    valid_data = ~(X.isna().any(axis=1) | y.isna())
    X = X[valid_data]
    y = y[valid_data]

    return X, y, features

def train_and_save_model():
    X, y, features = prepare_training_data()
    if X.empty or y.empty:
        print("No valid data after preprocessing. Please check your input data.")
        return

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('poly', PolynomialFeatures(degree=2, include_bias=False)),
        ('gb', GradientBoostingRegressor(random_state=42))
    ])

    param_distributions = {
        'gb__n_estimators': [100, 200],
        'gb__max_depth': [3, 4, 5],
        'gb__learning_rate': [0.01, 0.1, 0.2],
        'gb__subsample': [0.8, 1.0],
        'gb__min_samples_split': [2, 5],
        'gb__min_samples_leaf': [1, 2, 4]
    }

    tscv = TimeSeriesSplit(n_splits=5)
    random_search = RandomizedSearchCV(estimator=pipeline, param_distributions=param_distributions,
                                       n_iter=20, cv=tscv, n_jobs=-1, verbose=2, random_state=42)

    try:
        random_search.fit(X_train, y_train)

        print(f'Best Parameters: {random_search.best_params_}')
        print(f'Best Score: {random_search.best_score_}')

        best_pipeline = random_search.best_estimator_
        y_pred = best_pipeline.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f'Mean Squared Error: {mse}')
        print(f'R^2 Score: {r2}')

        joblib.dump(best_pipeline, 'best_model_pipeline.pkl')
        print("Model pipeline saved as 'best_model_pipeline.pkl'")
    except Exception as e:
        print(f"An error occurred during model training: {str(e)}")
        print("Please check your data and model configuration.")

def validate_model():
    X, y, features = prepare_training_data()
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    best_pipeline = joblib.load('best_model_pipeline.pkl')
    y_val_pred = best_pipeline.predict(X_val)

    mse_val = mean_squared_error(y_val, y_val_pred)
    r2_val = r2_score(y_val, y_val_pred)

    print(f'Validation Mean Squared Error: {mse_val}')
    print(f'Validation R^2 Score: {r2_val}')

def compare_feature_types(original_importances, division_importances, multiplication_importances, all_importances):
    plt.figure(figsize=(15, 10))

    feature_types = ['Original', 'Division', 'Multiplication', 'All']
    importances = [original_importances, division_importances, multiplication_importances, all_importances]
    colors = ['blue', 'green', 'red', 'purple']

    for i, (imp, color) in enumerate(zip(importances, colors)):
        top_10 = imp.head(10)
        x = np.arange(10) + i * 0.2
        plt.bar(x, top_10['Importance'], width=0.2, color=color, label=feature_types[i])

    plt.xlabel('Top 10 Features')
    plt.ylabel('Importance')
    plt.title('Comparison of Top 10 Features Across Different Feature Types')
    plt.xticks(np.arange(10) + 0.3, range(1, 11))
    plt.legend()

    plt.tight_layout()
    plt.show()

def print_features():
    X, y, features = prepare_training_data()
    print("Original features:")
    print(features)
    print("\nTotal number of features (including divisions):")
    print(X.shape[1])
    print("\nSample of division features:")
    division_features = [col for col in X.columns if '_div_' in col]
    print(division_features[:10])  # Print first 10 division features

if __name__ == "__main__":
    X, y, features = prepare_training_data()
    importances_df = analyze_feature_importance(X, y, features)
    print("Analyzing Original Features:")
    original_importances = analyze_feature_importance(X, y, features, feature_type='original')

    print("\nAnalyzing Division Features:")
    division_importances = analyze_feature_importance(X, y, features, feature_type='division')

    print("\nAnalyzing Multiplication Features:")
    multiplication_importances = analyze_feature_importance(X, y, features, feature_type='multiplication')

    print("\nAnalyzing All Features Combined:")
    all_importances = analyze_feature_importance(X, y, features, feature_type='all')

    compare_feature_types(original_importances, division_importances, multiplication_importances, all_importances)